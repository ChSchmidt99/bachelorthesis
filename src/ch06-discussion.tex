\section{Discussion}
\label{discussion}
\subsection{CPU vs GPU}
In general, CPUs are designed to execute serial instructions on an intermediate amount of data very fast, while GPUs are optimized to process instructions in parallel using little memory but maximizing throughput. Consequently, CPUs tend to utilize a bigger coherent cache than GPUs do and consist of few powerful cores that use techniques like pipelining, branch prediction and out-of-order execution to operate on data as efficiently as possible. GPUs, on the other hand, are built from many weaker cores that favor throughput over efficiency in individual cores, which makes graphic cards excel with highly coherent tasks. Conventional GPUs are optimized to run the graphics pipeline, with hardware units constructed specifically for tasks in the rasterization process. 

While path tracing is an embarrassingly parallel problem, the coherency of GPUs cannot be utilized to a full extend, as individual rays can often be very incoherent. Through the random nature of path tracing, rays might be scattered in all kinds of directions hitting different primitives in the process. In order to find ray intersections, it is also advantageous to have the whole acceleration structure in memory. With increasing scene complexity, this cannot always be provided by GPUs, given the limited amount of on board memory. Data transfer between CPU and GPU is costly and should be avoided when possible. 

Given those disadvantages, one goal of this thesis was to evaluate the performance of CPU path tracing. While interactive frame rates were achieved given reasonably small scenes and resolutions, the numbers did suggest that the presented CPU path tracer has not reached sufficient performance to be used as the sole rendering engine behind interactive, let alone real-time applications. This might be changed in the future though, as the tested implementation was only optimized moderately and still provides a lot of potential for further performance gains, especially through the use of SIMD instructions, better data layouts and further path tracing optimization methods. Moreover, the programming language used to implement this path tracer has a few shortcomings in terms of performance, which will be discussed in the next section (\nameref{programming_lang}).

Nevertheless, a better use case would be in the context of a hybrid path tracer where equivalent CPU and GPU implementations are combined. CPUs are mostly unused in real-time rendering, so the available capacity could be used to render frame subregions and relief the GPU that way. The BVH construction algorithm also showed promising results, so the CPU could be used mainly for the generation of acceleration structures. Note that these approaches are only hypothetical, but they provide interesting topics for future research nonetheless. 
\subsection{Programming Language}
\label{programming_lang}
The project was implemented in Go, an open source programming language originally aimed at networking applications. Consequently, the language provides great support for writing multi-threaded applications. This is facilitated through a lightweight thread management system in the form of so called goroutines\cite{Deshpande2012AnalysisOT}. These are not managed by the operating system but through Go's runtime scheduler, so they only exist in a virtual environment. Goroutines can be created very cheaply, as there initial stack only consists of a few kilo bytes. On blocking calls the scheduler automatically moves other goroutines to the blocked operating system thread. This keeps concurrent programming very efficient and simple, which was one of the main reasons for using Go. The language also offers great out of the box support for micro-benchmarking and benchmarking in general, which came in handy as well. 

However, while Go allows to write code somewhat close to the system, it is still a rather high level language. It uses a garbage collector that, even though very well optimized, is a substantial performance sink. Go's compiler also favors compile time over execution performance, so the performance is not comparable to faster programming languages. 

As a result, some optimizations were necessary to overcome some of Go's shortcomings. For example, many parts of the project are written with a zero allocation approach to avoid the need for expensive garbage collection. Structures are reused were possible and C style pointer parameter returns are used to avoid allocating return values. Slices, go's implementation of extendable arrays, are allocated with an appropriate capacity when available and reused when feasible. Even though Go supports pointers, arguments are often used as values which might sound counter intuitive at first. This is based on an issue mentioned by Changkun Ou\cite{ou20pointer} and has to do with different memory access patterns. Pointers need to be dereferenced, while the Go compiler is able to inline value calls to avoid moving values between registers. 

Despite Go being a relatively fast programming language, the focus on code readability over code performance is noticeable. Implementing the project in a faster language like C, C++ or Rust, might lead to better results, so translating the implementation into one of those languages and comparing their performance might be an interesting topic for future work. 
\subsection{Execution of the Optimization Step}
\label{discussion_execute}
While the grid search approach delivered more reliable results than Bayesian optimization, it is too costly to be used in interactive applications. Bayesian optimization had more competitive execution speeds, but its results were not optimal and fairly inconsistent. Whether adding the optimization time on top of the frame time actually improves the overall rendering performance depends on how ineffective the previous parameters were. Optimizing very malformed parameters might be worth it, but otherwise the optimization process adds useless computation complexity. However, there is also a lot of optimization potential left open, so this could be changed in the future.

Furthermore, the optimization time could be decreased by only executing it after a certain amount of time or frames has elapsed.  This makes sense, as only significant scene changes amount to a favorable parameter optimization. In practice, many parts of dynamic scenes stay static over large periods of time and only parts of the scene are actually changed. Consequently, running the optimization on every frame might not lead to any parameter changes anyways. However, efficiently executing parameter optimization in dynamic scenes remains an open topic.
\subsection{Ray Distribution Problem}
The surface area heuristic used to split cuts in PHR and evaluate the cost function proposed in section \ref{evaluation} assumes that ray origins and ray directions are uniformly distributed outside the scene's bounding box. This is a quite unrealistic assumption, especially in the context of real-time rendering where the camera often moves through a scene. One SAH correction proposed by Bittner and Havran\cite{bittner09rdh} is the \acrlong{rdh} based on sampling of the ray distribution:
\[
    P(N_c|N)^{RDH}=\frac{R(N_c)}{R(N)}
\]
where $R(N)$ is the number of rays hitting the bounding box of node $N$. Even though using RDH on its own might lead to unstable results due to under or oversampling, it would be an interesting topic to combine RDH with both the splitting process and the hyperparameter evaluation. Especially the latter could profit from such an evaluation, as the optimal PHR parameters differ between view points for each scene.
% TODO: Add figure that shows the correlation
This is not considered by the evaluation proposed in section \ref{evaluation}, as it is based on the SAH and suffers from the ray distribution problem.
\cleardoublepage